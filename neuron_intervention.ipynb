{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# textattack/facebook-bart-large-RTE\n",
    "# textattack/roberta-base-RTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaNeuralByPass(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        super(RobertaNeuralByPass, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.encoder = self.model.roberta.encoder\n",
    "        self.layer = self.encoder.layer\n",
    "        self.classifier = self.model.classifier\n",
    "        self.model.to(DEVICE)\n",
    "    \n",
    "    def tokenize(self, premise, hypothesis):\n",
    "        encoded = self.tokenizer(\n",
    "            premise, hypothesis,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return encoded\n",
    "    \n",
    "    def full_forward(self, encoded):\n",
    "        output = self.model(**encoded, output_hidden_states=True, output_attentions=True)\n",
    "        hidden_states = output.hidden_states\n",
    "        attentions = output.attentions\n",
    "        logits = output.logits\n",
    "        return logits, hidden_states, attentions\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor], \n",
    "        layer_id\n",
    "    ):\n",
    "        all_hidden_states = ()\n",
    "        for i, layer_module in enumerate(self.layer[layer_id-1:]):\n",
    "            layer_outputs = layer_module(\n",
    "                    hidden_states=hidden_states,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "            hidden_states = layer_outputs[0]\n",
    "            all_hidden_states += (hidden_states,)\n",
    "        \n",
    "        logits = self.classifier(hidden_states)\n",
    "        return logits, all_hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "roberta_bypass = RobertaNeuralByPass(\"roberta-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0, 3684, 3678,   32,  878,    2,    2, 6323, 3122,   32, 1375,    2]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = roberta_bypass.tokenize(\"All dogs are running\", \"Some animals are moving\")\n",
    "encoded.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4080,  0.8012,  2.9936]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, hidden_states, attentions = roberta_bypass.full_forward(encoded)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4079,  0.8012,  2.9936]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, all_hidden_states = roberta_bypass.forward(hidden_states[20], encoded['attention_mask'], 21)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ..., False, False, False]]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[21] == all_hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1519, -0.0937,  0.1447,  ..., -0.0300, -0.2084,  0.1826],\n",
       "         [-1.3912, -1.7311, -0.0307,  ..., -0.3199,  1.0177, -1.2630],\n",
       "         [-0.7188, -0.1330,  0.3086,  ..., -0.9791,  1.1518, -0.5150],\n",
       "         ...,\n",
       "         [-0.0531, -0.4782, -0.0294,  ..., -1.3526,  0.0126, -0.3265],\n",
       "         [-0.3398, -1.6078,  0.7697,  ...,  0.3044,  1.5741,  0.2432],\n",
       "         [-0.1567, -0.2220, -0.4250,  ..., -0.5397,  0.0649, -0.4106]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0528, -0.0299,  0.0852,  ..., -0.1457,  0.0252, -0.3061],\n",
       "         [-0.5096, -0.4977, -1.7494,  ...,  0.5328,  0.1078, -0.7403],\n",
       "         [-0.1172,  0.1241, -1.5735,  ...,  0.6089,  0.3702, -0.4649],\n",
       "         ...,\n",
       "         [-0.9167, -1.3176, -0.0505,  ..., -0.6160, -0.1814,  0.4936],\n",
       "         [-0.8177, -0.7924,  0.2921,  ..., -0.7807,  0.8872,  0.9065],\n",
       "         [-0.4522, -0.4103, -0.2238,  ..., -0.1327,  0.1258,  1.1909]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Lambda Notebook (Python 3)",
   "language": "python",
   "name": "lambda-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
